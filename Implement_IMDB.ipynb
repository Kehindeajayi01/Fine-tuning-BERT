{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cb3528",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ebb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fddf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b0ed8",
   "metadata": {},
   "source": [
    "## Checking for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8acc67",
   "metadata": {},
   "source": [
    "## Loading BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5197e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased',return_dict = False)\n",
    "\n",
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ba1c8",
   "metadata": {},
   "source": [
    "## Get and Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    # load the training and test sets\n",
    "    data = pd.read_csv(url, header = None, sep = \"\\t\")\n",
    "    data.columns = [\"label\", \"reviews\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_data(test_url):\n",
    "    test_data = get_data(test_url)\n",
    "    # split the training data to have validation sets\n",
    "    test_review, val_review, test_label, val_label = train_test_split(test_data[\"reviews\"], test_data[\"label\"], \n",
    "                                                        test_size = 0.2, stratify = test_data[\"label\"],random_state = 0)\n",
    "    return test_review, val_review, test_label, val_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809ec59",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df545572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_encode(train_url, test_url):\n",
    "    test_rev, val_rev, test_label, val_label = split_data(test_url)\n",
    "    train_data = get_data(train_url)\n",
    "    # tokenize and encode sequences in the training set\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "        train_data[\"reviews\"].tolist(),\n",
    "        max_length = 300,\n",
    "        padding = \"max_length\",\n",
    "        truncation=True)\n",
    "\n",
    "    # tokenize and encode sequences in the validation set\n",
    "    tokens_val = tokenizer.batch_encode_plus(\n",
    "        val_rev.tolist(),\n",
    "        max_length = 300,\n",
    "        padding = \"max_length\",\n",
    "        truncation=True)\n",
    "\n",
    "    # tokenize and encode sequences in the test set\n",
    "    tokens_test = tokenizer.batch_encode_plus(\n",
    "        test_rev.tolist(),\n",
    "        max_length = 300,\n",
    "        padding = \"max_length\",\n",
    "        truncation=True)\n",
    "    return tokens_train, tokens_val, tokens_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665032e8",
   "metadata": {},
   "source": [
    "## Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(train_url, test_url):\n",
    "    tokens_train, tokens_val, tokens_test = tokenize_encode(train_url, test_url)\n",
    "    test_review, val_review, test_label, val_label = split_data(test_url)\n",
    "    train_data = get_data(train_url)\n",
    "    \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(train_data[\"label\"].tolist())\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "    val_y = torch.tensor(val_label.tolist())\n",
    "\n",
    "    test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "    test_y = torch.tensor(test_label.tolist())\n",
    "    \n",
    "    # define a batch size\n",
    "    batch_size = 32\n",
    "\n",
    "    # wrap tensors\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "    # sampler for sampling the data during training\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "\n",
    "    # dataLoader for train set\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # wrap tensors\n",
    "    val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "    # sampler for sampling the data during validation\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "    # dataLoader for validation set\n",
    "    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_seq, test_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165542a",
   "metadata": {},
   "source": [
    "## BERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19dd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345a31a",
   "metadata": {},
   "source": [
    "## Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "\n",
    "# push the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)          # learning rate\n",
    "\n",
    "# define the loss\n",
    "cross_entropy = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbefac6",
   "metadata": {},
   "source": [
    "## Pre-train the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac315399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(train_url, test_url):\n",
    "    train_dataloader, _, _, _ = to_tensor(train_url, test_url)\n",
    "    model.train()   # set the model in training mode\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 200 == 0 and not step == 0:\n",
    "            print('Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu if available\n",
    "        batch = [r.to(device) for r in batch]\n",
    "    \n",
    "        sent_id, mask, labels = batch   # unpack the batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        #preds=preds.detach().cpu().numpy()   # use this if GPU is available\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49922663",
   "metadata": {},
   "source": [
    "## Evaluate the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe01db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(train_url, test_url):\n",
    "    \n",
    "    _, val_dataloader,_, _ = to_tensor(train_url, test_url)\n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    # deactivate dropout layers\n",
    "    model.eval()  # set the model in evaluation mode\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "                \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            #preds = preds.detach().cpu().numpy()  # use this if GPU is available\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de926384",
   "metadata": {},
   "source": [
    "## Fine-tune model on the IMDb datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(train_url, test_url, epochs):\n",
    "    # set initial loss to infinite\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    # empty lists to store training and validation loss of each epoch\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "\n",
    "    # for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "        \n",
    "        #train model\n",
    "        train_loss, _ = train(train_url, test_url)\n",
    "        \n",
    "        #evaluate model\n",
    "        valid_loss, _ = evaluate(train_url, test_url)\n",
    "        \n",
    "        #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "        \n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "        print(f'Validation Loss: {valid_loss:.3f}')\n",
    "        \n",
    "    return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"./datasets/train.csv\"\n",
    "test_url = \"./datasets/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, valid_losses = fine_tune(train_url, test_url, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793744c1",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a565ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(len(train_losses)))\n",
    "# Plotting\n",
    "plt.plot(epochs, train_losses, label = \"Train losses\")\n",
    "plt.plot(epochs, valid_losses, label = \"Validation losses\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Losses from the last layer in IMDb\")\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"losses-from-last-layer-imdb.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
